# Homegrown-NeuralNetwork

This project was done as a part of CSCI-B-551 Elements of Artificial Intelligence Coursework under Prof. Dr. David Crandall.

## Command to run the program ##

python3 main.py mlp

## Overview of Solution ##

In this solution firstly, I initialised the data i.e. setting _X and _y class variable with train data and one hot encodding of variables of labels respectively and also saving index of each category which we get after one hot encoding. Then in initialise funtion we are initialising weights and bias for input and output layer by taking matrix of dimmensions mentioned with random numbers. In fitted function  i called the initialised function firstly to initialise the data for model. Then in fit function, i ran a loop for number of iterations as passed in class to adjust weights and bias accordingly. In each iteration i am doing forward propogation process by multiplying the trainining data i.e. _X class variable with input layer weights i.e. _h_weights and adding input layer bias to it i.e. _h_bias and input layer output is formed. Then in next step i calculated the output layer output by calling hidden layer activation function on input layer output and multiplying it to output layer weights i.e. _o_weights and adding output layer bias to it i.e. _o_bias. Here forward propogation process ends. Then i am calculating final probabilities of each class by calling output layer activation function i.e. softmax on output layer output, this can be used for caculating loss after every 20 iterations. Then i am calculating error by subtracting actual label values i.e _y class variable from output layer  output. Then i am calculating output layer weight error by multiplying transpose matrix of output layer error to output of hidden layer activation function of input layer ouput and then taking transpose of whole matrix and then taking mean of each weight by dividing it by length of training data. Then i am calculating output layer bias error by taking sum of each whole column of output layer error and then taking mean error by dividing it by length of training data. After that i calculate the error of hidden layer by weights of output layer i.e. _o_weights and transpose of output layer error and then taking transpose of this whole multiplication. Also project this to derivate of hidden layer activation function for input layer output. And this final result will be input layer error. After this we have to caculate input layer weight error by multiplying transpose matrix of input layer error to input data _X and then taking transpose of whole matrix and then taking mean of each weight by dividing it by length of training data. At last i am caculating input layer bias error by taking sum of each whole column of input layer error and then taking mean error by dividing it by length of training data. After calculating each error we adjust input weights i.e. _h_weights by multiplying input layer weight error matrix by learning rate and subtracting it from _h_weights, then adjusting output weights i.e. _o_weights by multiplying output layer weight error matrix by learning rate and subtracting it from _o_weights, then adjusting input bias i.e. _h_bias by multiplying input layer bias error matrix by learning rate and subtracting it from _h_bias and atlast adjusting output bias i.e. _o_bias by multiplying output layer bias matrix by learning rate and subtracting it from _o_bias. I am substracting each because we take negative gradient decent to adjust weights and bias and this process is also called back propogation. Then after every 20 iterations i cacluclated loss history by calling loss function on final output probability matrix and actual y matrix and appending result to loss history matrix. I do all this things in each iteration till reach number of iteration set during class initialization and finally got final input and output layer weights and bias. The understanding of back propogation process was tough, i used andrew ng deep learning notes to understand it. In the predict function of multilayer perceptron, we initialise a empty list to get predicted y, then we iterate through each point in test data and do the forward propogation process i.e. multiplying test data with final input weights and adding bias to it. Then calling hidden layer activation on this output and then muliplying it by output weights and adding output bias to it. On this output we apply output layer activation function i.e. softmax to get final probabilities of each class. After that class with the highest probability is appended to predicted y list for that point. We do same process for every point in test data. The above mentioned activation can be calculated by different methods depending what is passed into class. The one activation function is identity function in which return the same matrix if we are not calculating derivative, if we are calculating derivative then we return one matrix of same dimension as x for which i have used numpy ones_like function. Other activation function is sigmoid in which i return 1/(1+e<sup>-x</sup>) if not caclculating derivative, if caculating derivating i return sigmod(x)*(1-sigmoid(x)), for exponential i used np.exp dunction of numpy. Third activation function is tanh for which i return tanh(x) if not derivative otherwise i reurned 1 - tanh(x)<sup>2</sup>, for caculating tanh i used tanh function of numpy. Last activation was relu for which if not derivative i return array with 0 if value was less than 0 otherwise same value, if derivative i returned array with 0 if value was less than 0 otherwise 1, also for this i used numpy functionality of where. Softmax activation was already implemented in skeleton code. I also implemented cross entropy function for calculating loss, in which we do a negative summation each cell of actual y multiplied by log(predicted y) for that cell. The final function in util.py which was implemented was one hot encoding for which i found different unique categories present list and assign a index to them. Then I iterate through each y and and make list of length of unique categories with all zeroes only making 1 the index which was given to particular category i.e. y and append this to a list anf did the same thing for every point in y.  The multilayer_perceptron class is called from main.py with different combinations of number of iterations, activation functions and learning rate, with train and test data for each combination an accuracy is calculated and stored in html file.

## Accuracy ##

Accuracy for my multilayer perceptron  model is very low as compared sklearn model.

## Challenges ##

Main challenge in this part was to understand the concept of back propogation which was really a big task for me.

